{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-message",
   "metadata": {},
   "source": [
    "## Objectives - Build an ETL pipeline for a data lake hosted on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-third",
   "metadata": {},
   "source": [
    "* Load data from S3\n",
    "* Process the data into analytics tables using Spark\n",
    "* Load them back into S3\n",
    "* Deploy the Spark process on an AWS cluster using AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-metabolism",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleasant-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"create a sparksession and return it to user\"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_my_tables():\n",
    "    \"\"\"drop all created tables and views\"\"\"\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS logs_json;\"\"\")\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS songs_json;\"\"\")\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS songs;\"\"\")\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS artists;\"\"\")\n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS users;\"\"\")    \n",
    "    spark.sql(\"\"\"DROP TABLE IF EXISTS users;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"parse song data to create songs and artists tables, write to parquet\"\"\"\n",
    "    # get filepath to song data file\n",
    "    songs_path = input_data + 'song_data'\n",
    "\n",
    "    # read song data file\n",
    "    songs_df = spark.read.option(\"recursiveFileLookup\", \"true\").json(songs_path)\n",
    "    songs_df.createOrReplaceTempView(\"songs_json\")\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    spark.sql(\"CREATE TABLE songs AS SELECT song_id, title, artist_id, year, duration FROM songs_json;\")\n",
    "    songs_table = spark.sql(\"\"\"SELECT * FROM songs;\"\"\")\n",
    "    spark.sql(\"\"\"SELECT * FROM songs limit 5;\"\"\").collect()\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy(\"year\", \"artist_id\").mode(\"overwrite\").parquet(output_data + \"songs/songs_table.parquet\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    spark.sql(\"\"\"CREATE TABLE artists AS SELECT artist_id, artist_name AS name, artist_location AS location, artist_latitude AS latitude, artist_longitude AS longitude FROM songs_json;\"\"\")\n",
    "    artists_table = spark.sql(\"\"\"SELECT * FROM artists;\"\"\")\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.partitionBy(\"name\").mode(\"overwrite\").parquet(output_data + \"artists/artist_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    logs_path = input_data + '/logs/*.json'\n",
    "\n",
    "    # read log data file\n",
    "    logs_df = spark.read.json(logs_path)\n",
    "    logs_df.createOrReplaceTempView(\"logs_json\")\n",
    "\n",
    "    # extract columns for users table - filtering by actions for song plays\n",
    "    spark.sql(\"\"\"CREATE TABLE users AS SELECT userId AS user_id, firstName AS first_name, \\\n",
    "            lastName AS last_name, gender, level FROM logs_json WHERE page = 'NextSong';\"\"\")\n",
    "    users_table = spark.sql(\"\"\"SELECT * FROM users;\"\"\")\n",
    "    # write users table to parquet files\n",
    "    users_table.write.partitionBy(\"user_id\").mode(\"overwrite\").parquet(output_data + \"users/users_table.parquet\") \n",
    "\n",
    "    #create multiple udfs for time handling \n",
    "    #python datetime expects seconds, not milliseconds\n",
    "    spark.udf.register(\"get_timestamp\", lambda x: int(x))\n",
    "    spark.udf.register(\"get_day\", lambda x: datetime.fromtimestamp(x/1000.0).day)\n",
    "    spark.udf.register(\"get_hour\", lambda x: datetime.fromtimestamp(x/1000.0).hour)\n",
    "    spark.udf.register(\"get_week\", lambda x: datetime.fromtimestamp(x/1000.0).isocalendar().week)\n",
    "    spark.udf.register(\"get_month\", lambda x: datetime.fromtimestamp(x/1000.0).month)\n",
    "    spark.udf.register(\"get_year\", lambda x: datetime.fromtimestamp(x/1000.0).year)\n",
    "    spark.udf.register(\"get_weekday\", lambda x: datetime.fromtimestamp(x/1000.0).weekday())\n",
    "    \n",
    "    # extract columns to create time table - filtering by actions for song plays\n",
    "    spark.sql(\"\"\"CREATE TABLE time AS SELECT get_timestamp(ts) AS start_time, get_hour(ts) AS hour, get_day(ts) AS day,\\\n",
    "        get_week(ts) AS week , get_month(ts) AS month, get_year(ts) AS year , \\\n",
    "        get_weekday(ts) AS weekday FROM logs_json WHERE page = 'NextSong';\"\"\")\n",
    "    time_table = spark.sql(\"\"\"SELECT * FROM time;\"\"\")\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_data + \"times/time_table.parquet\")\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table - filtering by actions for song plays\n",
    "    spark.sql(\"\"\"CREATE TABLE songplays AS \\\n",
    "                        (SELECT l.ts AS start_time, t.year AS year, t.month AS month,\\\n",
    "                        l.userId AS user_id, l.level AS level, \\\n",
    "                        s.song_id AS song_id, s.artist_id AS artist_id, \\\n",
    "                        l.sessionId AS session_id, \\\n",
    "                        s.artist_location AS location, \\\n",
    "                        l.userAgent as user_agent \\\n",
    "                        FROM time AS t\n",
    "                        JOIN logs_json AS l ON t.start_time = l.ts AND l.page = 'NextSong' \\\n",
    "                        JOIN songs_json AS s ON s.artist_name = l.artist);\"\"\")                    \n",
    "    songplays_table = spark.sql(\"\"\"SELECT * FROM songplays;\"\"\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_data + \"songplays/songplays_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I AM SAVED!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "cusum = df.filter((df.page == 'NextSong') | (df.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', function(col('page'))) \\\n",
    "    .withColumn('period', Fsum('homevisit').over(user_window))\n",
    "\n",
    "cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'}) \\\n",
    "    .agg({'count(period)':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd09fdc14dc46aa34dae070d24814aa0b4975e1e5d5386029cc1e3d2a2d730baa3d",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}